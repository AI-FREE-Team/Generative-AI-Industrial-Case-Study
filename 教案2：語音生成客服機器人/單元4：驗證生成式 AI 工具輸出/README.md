# 單元四、生成式 AI 工具輸出驗證方法

## 1. 前言
to-do

## 2. 以 RAG 交叉驗證大型語言模型輸出內容
我們曾經在**教案一、生成式AI基礎**的**第四單元、生成式AI工具使用須知**當中提及檢索增強生成（Retrieval-Augmented Generatrion，RAG）的技術，其主要的目的是為了解決語言模型的幻覺行為（Hallucination），如果讀者對於這兩個專有名詞的介紹感到陌生，歡迎參考[教案一的內容介紹](https://github.com/AI-FREE-Team/Generative-AI-Industrial-Case-Study/tree/main/%E6%95%99%E6%A1%881%EF%BC%9A%E7%94%9F%E6%88%90%E5%BC%8F%20AI%20%E5%9F%BA%E7%A4%8E/%E5%96%AE%E5%85%833%EF%BC%9A%E7%94%9F%E6%88%90%E5%BC%8F%20AI%20%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E9%A0%88%E7%9F%A5)。

### 2.1 減緩幻覺行為的發生
根據單元三實際展示的畫面，我們所聽到的語音助理所產生的回覆（聲音），其實就是根據GPT等大型語言模型（LLM）輸出的文字，然後再利用一定程序的自然語言處理（NLP）技術，規則化/簡化內容後，再利用文字轉語音（Text-to-Speech）的技術才生出來的。

因此如果GPT等大型語言模型（LLM）在輸出的文字當中，有因為幻覺行為的出現而導致不實的資訊產生，則可能造成客戶的不便；而RAG的出現則可以有效緩解此情況。讀者可以把RAG想作是LLM在輸出文字的最後一階段，會再回頭參考我們特地為LLM準備的「正確的」文字資訊，因此LLM會在輸出文字時，再一次根據我們準備的外部資訊調整最終輸出內容。

此舉將有效降低大型語言模型幻覺行為的出現情況。

### 2.2 根據回覆提供參考索引（Reference）
除了能夠減緩幻覺行為的出現，RAG之所以能夠大幅增加GPT系列等大型語言模型的落地可能性，是因為配合上RAG的技術之後，能夠在回覆時一併提供使用者它所參考的參考索引。這說明使用者如果對於AI回覆內容存在疑慮時，能夠透過AI一併附上的參考連結（可能是連結到常見的QA問答集當中），進一步的確認資訊。

## 3. 驗證聲音輸出的正確性

### 3.1 服務上架前：使用情境（Usecase）驗證
通常文字生成語音（Text-to-Speech）這步驟通常比較不會出現問題，不過為了驗證AI機器人生成文字的正確性，通常會需要在正式上線系統之前先行測試。讀者這邊若有需求，建議需要針對5個最常見的情境設定測試的使用情境（Usecase），然後實際地利用AI智能助理輸出音訊，感受其聲音品質與文字的辨別度。

### 3.2 服務上架後：ASR 自動化辨識聲音
然而在服務上架之後，就比較無法使用3.1這種人為介入程度高的驗證方式進行，會需要建立自動化的驗證方案。此時，其中一個自動化的驗證方案就是利用自然語言處理（NLP）其中的ASR技術，應用人工智慧的技術自動分析音訊，輸出該段聲音所對應的文字，這時我們可以再將ASR預測的結果（也就是輸出的文字）與TTS（文字轉語音）的輸入（同樣也是文字）進行匹配。

舉例而言，如果匹配程度低於99%，則需要主動警告軟體部門開發人員回頭檢查匹配率低的文字情境，此時再行系統優化。