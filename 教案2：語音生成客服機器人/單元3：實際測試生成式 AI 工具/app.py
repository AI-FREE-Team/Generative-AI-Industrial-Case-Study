import os
import whisper
import streamlit as st
from pydub import AudioSegment
import warnings
from st_audiorec import st_audiorec
from openai import OpenAI

# å¿½ç•¥ç‰¹å®šç±»å‹çš„è­¦å‘Š
warnings.filterwarnings("ignore")

st.set_page_config(
    page_title="Whisper based ASR",
    page_icon="musical_note",
    layout="wide",
    initial_sidebar_state="auto",
)

audio_tags = {'comments': 'Converted using pydub!'}

upload_path = "uploads/"
download_path = "downloads/"
transcript_path = "transcripts/"

# @st.cache(persist=True,allow_output_mutation=False,show_spinner=True,suppress_st_warning=True)
def to_mp3(audio_file, output_audio_file, upload_path, download_path):
    ## Converting Different Audio Formats To MP3 ##
    if audio_file.split('.')[-1].lower()=="wav":
        audio_data = AudioSegment.from_wav(os.path.join(upload_path,audio_file))
        audio_data.export(os.path.join(download_path,output_audio_file), format="mp3", tags=audio_tags)

    elif audio_file.split('.')[-1].lower()=="mp3":
        audio_data = AudioSegment.from_mp3(os.path.join(upload_path,audio_file))
        audio_data.export(os.path.join(download_path,output_audio_file), format="mp3", tags=audio_tags)
    return output_audio_file

# @st.cache(persist=True,allow_output_mutation=False,show_spinner=True,suppress_st_warning=True)
def process_audio(filename, model_type):
    model = whisper.load_model(model_type)
    result = model.transcribe(filename)
    return result["text"]

def whisper_infer(wav_path):
    transcript = process_audio(wav_path , 'base')
    return transcript

st.title("ğŸ—£Streamlit ASR + GPT + TTS âœ¨")
st.info('âœ¨ Supports all popular audio formats - WAV, MP3, MP4, OGG, WMA, AAC, FLAC, FLV ğŸ˜‰')
st.info('è«‹è¼¸å…¥éŸ³æª”æˆ–æ˜¯ç›´æ¥èªªè©±')

uploaded_file = st.file_uploader("Upload audio file", type=["wav","mp3","ogg","wma","aac","flac","mp4","flv"])
radio_bytes = st_audiorec()

audio_file = None
transcript = None
response = None

if uploaded_file is not None:
    audio_bytes = uploaded_file.read()
    with open(os.path.join(upload_path,uploaded_file.name),"wb") as f:
        f.write((uploaded_file).getbuffer())
    with st.spinner(f"Processing Audio ... ğŸ’«"):
        output_audio_file = uploaded_file.name.split('.')[0] + '.mp3'
        audio_file = uploaded_file.name
        output_audio_file = to_mp3(audio_file, output_audio_file, upload_path, download_path)
        audio_file = open(os.path.join(download_path,output_audio_file), 'rb')
        audio_bytes = audio_file.read()
    st.markdown("---")
    st.markdown("Feel free to play your uploaded audio file ğŸ¼")
    st.audio(audio_bytes)
    wav_path = str(os.path.abspath(os.path.join(download_path,output_audio_file)))
    if st.button("Generate Transcript"):
        with st.spinner(f"Generating Transcript... ğŸ’«"):
            st.write("Transcription Result:")
            transcript = whisper_infer(wav_path)
            st.write(transcript)
            st.success('âœ… Recoginize Successful !!')

if radio_bytes is not None:
    wav_name = 'test.wav'
    with open(os.path.join(upload_path,wav_name),"wb") as f:
        f.write(radio_bytes)
    with st.spinner(f"Processing Audio ... ğŸ’«"):
        output_audio_file = wav_name.split('.')[0] + '.mp3'
        output_audio_file = to_mp3(wav_name, output_audio_file, upload_path, download_path)
        audio_file = open(os.path.join(download_path,output_audio_file), 'rb')
        audio_bytes = audio_file.read()
    wav_path = str(os.path.abspath(os.path.join(download_path,output_audio_file)))
    if st.button("Generate Radio Transcript"):
        with st.spinner(f"Generating Transcript... ğŸ’«"):
            st.write("Transcription Result:")
            transcript = whisper_infer(wav_path)
            st.write(transcript)
            st.success('âœ… Recoginize Successful !!')

if transcript is not None:
    client = OpenAI(
        api_key = 'è«‹æ”¹æˆè‡ªå·±çš„OpenAI API KEY'
    )

    # question = 'å°å¹£æ€éº¼æ›æˆç¾å…ƒ'
    entity = 'è­‰ä»¶æº–å‚™ã€é¸æ“‡éŠ€è¡Œèˆ‡å¸³æˆ¶é¡å‹ã€å¡«å¯«ç”³è«‹è¡¨æ ¼ã€æäº¤ç”³è«‹ã€ç­‰å¾…å¯©æ ¸ã€å­˜æ¬¾ã€è¨­å®šç¶²è·¯éŠ€è¡Œèˆ‡è¡Œå‹•éŠ€è¡Œ'

    qqq = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é‡‘èç›¸é—œçš„äººå“¡\
    è«‹ä½ å®Œæˆå›è¦†é‡‘èç›¸é—œçš„çŸ¥è­˜ \
    ä»¥ä¸‹æ˜¯æå•çš„å•é¡Œ\n {transcript}\
    è«‹å›ç­”ä¸Šè¿°å•é¡Œ\
    è«‹å¹«æˆ‘æŠŠå°ˆæœ‰åè©æŠ“å‡ºä¾† å›è¦†æ™‚è«‹æ›è¡Œ ä¾‹å¦‚:{entity} å†é¡å¤–æ›è¡Œ\
    """
    with st.spinner(f"Generate Response... ğŸ’«"):
        completion = client.completions.create(
            model="text-davinci-003",
            prompt=qqq,
            max_tokens=512,
            temperature=0.5,
            )

        response = completion.choices[0].text
        with st.spinner(f"éŠ€è¡Œæ©Ÿå™¨äººå°ˆæ¥­å›è¦†"):
            st.write(response)
    with st.spinner(f"Generate Speech... ğŸ’«"):
        speech_serve = client.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=response
        )
        print(speech_serve)
        speech_serve.stream_to_file(os.path.join(download_path , 'speech.mp3'))
        tts_file = open(os.path.join(download_path,'speech.mp3'), 'rb')
        tts_bytes = tts_file.read()
    st.audio(tts_bytes)
    st.success('âœ… TTS Output !')